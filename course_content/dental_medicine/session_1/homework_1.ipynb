{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce87138",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "# Homework Task 1: HPV Status and Head and Neck Squamous Cell Carcinoma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0eb151",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## Clinical Background: Identifying HPV Status from RNA Data in Head and Neck Cancer\n",
    "\n",
    "Human papillomavirus (HPV) is a key etiological factor in a subset of head and neck squamous cell carcinomas (HNSCC), particularly those arising in the oropharynx[1]. HPV-positive HNSCC is clinically and biologically distinct from HPV-negative disease[2]. Patients with HPV-positive tumors generally have a better prognosis and respond more favorably to treatment[3], which has led to the development of de-escalation strategies in clinical trials[4]. Therefore, accurate determination of HPV status is critical for diagnosis, prognosis, and treatment planning.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f8b078",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "### Traditional Detection Methods\n",
    "\n",
    "Clinically, HPV status is typically determined through p16 immunohistochemistry (as a surrogate marker), in situ hybridization, or PCR-based methods for detecting viral DNA or RNA[5]. However, with the increasing availability of high-throughput sequencing data, computational approaches now offer alternative ways to infer HPV status using RNA expression profiles[6]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25eb50b",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "### Why Use RNA-seq Data?\n",
    "\n",
    "RNA sequencing (RNA-seq) provides a comprehensive snapshot of gene expression levels across the entire transcriptome. Tumors driven by HPV often exhibit distinct transcriptional signatures, not only due to the presence of viral transcripts, but also because HPV-related oncogenesis affects host gene expression patterns[7]. This makes RNA-seq a valuable tool for distinguishing between HPV-positive and HPV-negative cancers using machine learning techniques[8]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837da19",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "### About the Dataset: TCGA and Pan-Cancer Atlas\n",
    "\n",
    "The data used in this task comes from **The Cancer Genome Atlas (TCGA)**, specifically the **Pan-Cancer Atlas** collection[9]. TCGA is a landmark cancer genomics program that has molecularly characterized over 20,000 primary cancer and matched normal samples across 33 cancer types. The Pan-Cancer Atlas is an integrated dataset that combines genomic, transcriptomic, and clinical information across multiple cancer types, including head and neck squamous cell carcinoma (HNSC)[10].\n",
    "\n",
    "For this assignment, you'll be working with **RNA-seq data** from **head and neck cancer biopsies** in TCGA, where the **HPV status** of each patient has been annotated. The goal is to use these RNA profiles to build a model that can **classify patients as HPV-positive or HPV-negative** based on their gene expression patterns.  \n",
    "  \n",
    "We have pre-processed the data to make the task more enjoyable. Whilst RNA-seq can pick up over 24,000 genes, we've taken only 1000 of them. We've also normalised the gene expression levels so that they all appear between 0 and 1. Machine learning methods work better when all of the features (genes in this case) have values that are on the same scale as each other (rather than some being much larger or smaller than others!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa68ac",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## The Task\n",
    "\n",
    "The assignment here is to use the RNA profiles provided to build a machine learning model that will be able to classify patients as HPV-positive or HPV-negative based on their gene expression patterns.  \n",
    "  \n",
    "We will use a **random forest algorithm** to do this. Random forests are a great, general-purpose machine learning algorithm when you are dealing with **tabular data**. This is data that can be conveniently written down in a table (like a spreadsheet).  \n",
    "  \n",
    "More complicated machine learning methods, such as neural networks, tend to be too complex to work well in tabular data. They often end up fitting noise in the data, rather than overall trends, leading to them performing worse than random forests.\n",
    "\n",
    "### Checklist\n",
    "The tasks are as follows: \n",
    "\n",
    "1) Import the libraries you need for your work\n",
    "2) Load in the dataset that you have been provided\n",
    "3) Perform train-test splitting to prepare the data for analysis\n",
    "4) Build a random forest model to do the classification task\n",
    "  \n",
    "Wherever you see a cell with \"(Help)\", you can ignore it if you want! It is just to provide support/help if you are stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd4dfe",
   "metadata": {},
   "source": [
    "## If you get stuck\n",
    "  \n",
    "A key part of coding is getting stuck and working out the problem. If you get stuck, the following things may help you: \n",
    "* read the error message that is being produced \n",
    "* google the error that you are getting: maybe someone else has had it before\n",
    "* use a chatbot like ChatGPT or Deepseek to try to fix your problem\n",
    "* look at the demonstration notebook that carried out a very similar problem\n",
    "  \n",
    "\n",
    "We **strongly urge** that you try to fix your problems using other methods before getting ChatGPT (or other chatbot) to fix them. Whilst it will probably work, it will not help you learn as much. If you try to solve the problem yourself, you will be better at fixing it if it ever happens again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acc655",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## 1) Import the libraries you need for your work  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa30885",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "The first task is *always* to import the libraries you need for your work. This is the same as `library(dplyr)` in `R`. Libraries are the fundamental unit of reproducible code. It prevents you having to redo something that you've done before, and allows you to share solutions with other people.  \n",
    "  \n",
    "Remember in `Python`, there are two ways that you can import things\n",
    "1) `import numpy as np`  \n",
    "    * This imports the `numpy` library, so if you want any function from inside `numpy`, you use `np.function()`  \n",
    "    * This is useful if you are going to use a lot of functions that all come from the same library.  \n",
    "    * The \"as\" here is the name that we are going to use for the library in our code. It's convenient to use a short version. For `numpy`, we normally use `np`. For `pandas`, we use `pd`. You can use anything you want though\n",
    "  \n",
    "2) `from numpy import sum`  \n",
    "    * This imports the `sum` function from inside the `numpy` library.  \n",
    "    * This is useful if you are using functions that coming from different parts of the same library (e.g. in `sklearn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7518da9",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "For this task, you are going to need the following libraries: \n",
    "* the `matplotlib.pyplot` library \n",
    "* the `pandas` library\n",
    "* the `numpy` library \n",
    "* the function `RandomForestClassifier` from the `sklearn.ensemble` library\n",
    "* the function `train_test_split` from the `sklearn.model_selection` library\n",
    "  \n",
    "Write Python code in the cell below to import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "137dc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your Python code in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512ed0d",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## 2) Load in the dataset that we are going to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6837c",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "Now we need to load in the dataset that we are going to be working with. The file that we are going to install is in the `dataset` folder and is called `hnsc_dataset_scaled.csv`.  \n",
    "  \n",
    "A `.csv` file is a common file format used by data scientists. It is a \"comma separated values\" file. If you try to open this file, you would see all of the data separated by commas \",\". This is a file format that is used in Excel Spreadsheets as well. \n",
    "  \n",
    "To read in a `.csv` file, we use the `read_csv()` function from the `pandas` library.  \n",
    "  \n",
    "Remember, if you used `import pandas` in the previous section, then you will need to change this from `pd.read_csv` to `pandas.read_csv`\n",
    "  \n",
    "In the cell below, use `read_csv()` to read in the dataset. Inside the brackets, put the path to the file in speech marks. The \"path\" tells the code how to get from the `.ipynb` notebook where it is currently running to where the data that it is going to read is.  \n",
    "  \n",
    "For us, that path is `\"dataset/hnsc_dataset_scaled.csv\"`. Replace the `\"replace_this_with_file_name\"` in the next cell with this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3229576b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'replace_this_with_file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Put in the path to the file here\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace_this_with_file_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/envs/precision-course-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/envs/precision-course-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/envs/precision-course-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/envs/precision-course-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/envs/precision-course-env/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'replace_this_with_file_name'"
     ]
    }
   ],
   "source": [
    "### Put in the path to the file here\n",
    "\n",
    "df = pd.read_csv(\"replace_this_with_file_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34b7b0",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "It is good practice to check that the data you have is what you expected. Remember that you can use `df.head()` to show the top 5 rows of the dataset.  \n",
    "  \n",
    "Use that here, check that the data seems to be read in sensibly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6716627",
   "metadata": {},
   "outputs": [],
   "source": [
    "### View the first 5 rows of your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c0d27",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "* Does the data the correct sensible number of rows and columns?  \n",
    "  \n",
    "* We are expecting 1000 columns for the 1000 genes, 1 column for the HPV-status (positive/negative).  \n",
    "  \n",
    "* How many patients are there?\n",
    "  \n",
    "* To view the shape of the dataset (number of rows and columns), you can use `df.shape` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5491884",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print out the shape of the dataset here. Is it what you expected? How many patients are there?\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98766f12",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "We need to separate the outcome that we are trying to predict (HPV-status) from the features (the gene expression levels) that we are going to use to predict it. \n",
    "  \n",
    "In the cell below, create new variables `X` and `y`.  \n",
    "* `y` should be the `HPV_Status` column from `df`\n",
    "* `X` should have only the columns that include gene expression information\n",
    "  \n",
    "Remember, in pandas, if you have a dataset called `data`, then you can get the column called \"`outcome`\", by using `data[\"outcome\"]`. To get all columns that are *not* called \"outcome\", you could use `data.drop(\"outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59933e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the outcome and training information here \n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a5661",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## 3) Train-Test Splitting  \n",
    "  \n",
    "As was explained in the lecture, train-test splitting is a crucial part of building machine learning models.  \n",
    "The core task in machine learning is to build a model/algorithm that will predict the quantity of interest on new samples that it hasn't seen before.  \n",
    "In our case, we want to build a machine learning model that can predict an HNSC patient's HPV status based on their gene expression levels.  \n",
    "  \n",
    "We use a **training set** to build the model. However, because the model has seen this code before, the performance on this training data is a *biased* estimate of how the model would perform on new data. We also need a **testing set** that we *only* use right at the end to evaluate the model. We will discuss model evaluation in the next session.  \n",
    "  \n",
    "For the time being, we will just do train-test splitting here. The scikit-learn library in `Python` provides tools for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84accd22",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "************************************************************************************************************************************************************************************************************   \n",
    "  \n",
    "(HELP)  \n",
    "  \n",
    "Let's say that you had a dataset called `data` and a quantity you were trying to predict called `outcome`. You would do train-test splitting using `scikit-learn` with the following code: \n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, outcome, test_size = 0.01, random_state = 50)\n",
    "```  \n",
    "Here, I've used a `test_size` of 0.01 (meaning 1% of my data is used for the testing set, and 99% is used for the training set) and a `random_state` of 50.  \n",
    "  \n",
    "The `random_state` is a variable that allows my splitting to be reproduced by other people. Every time this code is run, no matter the computer, if `random_state` is not changed, then the train test splitting will be identical\n",
    "************************************************************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b68abb",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "In the following cell, do train-test splitting of the HNSC dataset. Use a test_size of 0.2 and a random_state of 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1666e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do train-test splitting here\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c050f0",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "In the \"Markdown\" cell below, answer the following questions. You can type `1)` and then write your answer to have them appear\n",
    "\n",
    "1) Why is it important to do train-test splitting?\n",
    "2) What should you do with the test set once you have produced it?\n",
    "3) The `random_state` variable in the `train_test_split()` function ensures reproducibility: we get the same train-test split every single time we run the code, why might this be important?\n",
    "4) My model isn't working very well, what if I used the testing set for more training, and then used it again to test? Would this be incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f67f7",
   "metadata": {},
   "source": [
    "#### Your Answers  \n",
    "1)  \n",
    "  \n",
    "2)  \n",
    "   \n",
    "3)  \n",
    "   \n",
    "4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f2cf9",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## 4) Building a Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae3a04",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "Finally, we get to actually building our machine learning model.  \n",
    "  \n",
    "As discussed before, we are going to use \"random forest\". This is a great, general-purpose machine learning model that should be good for most purposes. \n",
    "  \n",
    "The actual algorithm was discussed in the lecture. If you want some more information, feel free to check out some of the following resources: \n",
    "* StatQuest with Josh Starmer: https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "* Stemplicity: https://www.youtube.com/watch?v=Y85XV45x0VU\n",
    "* Victor Zhou: https://victorzhou.com/blog/intro-to-random-forests/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db5902",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "We are *not* going to ask you to implement a random forest yourself.  \n",
    "  \n",
    "In general, unless you are inventing new machine learning algorithms, it is almost always a bad idea to try to implement an algorithm completely from scratch yourself.  \n",
    "  \n",
    "For personalised medicine, we will always use versions of these algorithms that someone has already created and saved as a library (like `scikit-learn`).  \n",
    "This is for a few reasons:   \n",
    "* the library version has been checked and validated by other people, so it is more likely to be error-free\n",
    "* the library version has been optimised and written efficiently, so it will be quicker than any version we can write\n",
    "* the library version is *safe*. The code is saved open-source online, so we know exactly what it is doing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b408ebe",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "For random forest, we are going to use the `RandomForestClassifier` function from the `sklearn.ensemble` library. You should have imported that earlier in this notebook.  \n",
    "  \n",
    "The next cell will use the python `help` function to show the help page for RandomForestClassifier. You will see that it contains a lot of information! The same information is summarised here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html  \n",
    "  \n",
    "*You do not need to read all this information*  \n",
    "  \n",
    "We are including it to show how many parameters there are that go into the `RandomForestClassifier` algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a4395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
      " |  `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  For a comparison between tree-based ensemble models see the example\n",
      " |  :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      " |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      " |      Note: This parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      " |        split.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool or callable, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
      " |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      " |      custom metric. Only available if `bootstrap=True`.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`Glossary <warm_start>` and\n",
      " |      :ref:`gradient_boosting_warm_start` for details.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  monotonic_cst : array-like of int of shape (n_features), default=None\n",
      " |      Indicates the monotonicity constraint to enforce on each feature.\n",
      " |        - 1: monotonic increase\n",
      " |        - 0: no constraint\n",
      " |        - -1: monotonic decrease\n",
      " |  \n",
      " |      If monotonic_cst is None, no constraints are applied.\n",
      " |  \n",
      " |      Monotonicity constraints are not supported for:\n",
      " |        - multiclass classifications (i.e. when `n_classes > 2`),\n",
      " |        - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
      " |        - classifications trained on data with missing values.\n",
      " |  \n",
      " |      The constraints hold over the probability of the positive class.\n",
      " |  \n",
      " |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      " |  \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator. Each subset is defined by an array of the indices selected.\n",
      " |  \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
      " |      Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
      " |      10_000).\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |      \n",
      " |      Returns a dynamically generated list of indices identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |      \n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bdd9d",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "Most machine learning algorithms have **hyperparameters**. These are parameters that affect how the algorithm behaves, and can change how accurately it can classify data. However, they are not learned directly from the training data. They are parameters that you (the user) needs to set manually.  \n",
    "  \n",
    "**Hyperparameter Optimisation** is the task of picking these hyperparameters so that our machine learning algorithm performs as well as it possibly can. It is an important part of using machine learning models. However, we are not going to handle it in this course. If you are interested, you can check out this link (https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/).  \n",
    "  \n",
    "For random forests, the most important hyperparameters are: \n",
    "* `random_state`: As with train-test splitting, a number that ensures our model is reproducible every time we run the code \n",
    "* `n_estimators`: The number of decision trees in our random forest. Normally, 100 is a sensible number. \n",
    "* `max_depth`: How deep each decision tree in the forest is allowed to go before we build a new one. Normally, 10 is a sensible number. \n",
    "  \n",
    "There are other hyperparameters too, but we will not adjust these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c145e04",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "In the cell below, build a random forest model using `RandomForestClassifier`. For its hyperparameters, use: \n",
    "* `random_state = 42`\n",
    "* `n_estimators = 100`\n",
    "* `max_depth = 10`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb98666",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">  \n",
    "\n",
    "********************************************************************************************************* \n",
    "(HELP)  \n",
    "If I was trying to build a `Forest` model with a hyperparameter called `Size` with value 10, then I would use this code: \n",
    "```\n",
    "model = Forest(Size = 10)\n",
    "```\n",
    "********************************************************************************************************* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build your model here\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e047ab",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "Now it is time to train your model. This involves providing examples to the model that it can use to adjust its parameters and produce the best algorithm for classifying samples. \n",
    "  \n",
    "For this process, we provide the model both:\n",
    "* training data (i.e. the `X_train` from earlier that contains gene expression levels)\n",
    "* labels which tell the model what the prediction should be for each of the training examples (i.e. the `y_train` from earlier)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111c447",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "Machine learning models \"learn\" by seeing training examples and outcomes. They infer patterns in the training examples and associate these with the outcomes. When we give them new examples, they try to find those patterns and \"predict\" an outcome based on what they've seen before.  \n",
    "  \n",
    "The more training examples that a model is provided, the better these patterns will be and the better the model will perform.  \n",
    "  \n",
    "This is why you will often hear that machine learning models \"need more data\". It is about providing more training examples to reinforce those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c204762",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "In the cell below, train your machine learning model  \n",
    "  \n",
    "You can do this by calling `model.fit(a, b)`. `a` should be your training examples (e.g. `X_train` from above) and `b` should be your outcomes for each of the training examples (e.g. `y_train` from above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ed081",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Train your machine learning model here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/envs/precision-course-env/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "### Train your machine learning model here\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd872c",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "# Summary and Next Steps\n",
    "  \n",
    "In this homework, you have learned how to: \n",
    "* import python libraries for data science\n",
    "* load a dataset and prepare it for model building with train-test splitting \n",
    "* build and train a random forest machine learning model \n",
    "  \n",
    "This workflow is the same for most machine learning applications, so these are important stages to understand.  \n",
    "  \n",
    "## Next Session \n",
    "  \n",
    "A course instructor will go through this completed workbook with you, offering feedback and checking your work.  \n",
    "  \n",
    "We have only built the model, not tested how well it works. This is the process of \"model evaluation\". We will discuss this in detail in the next session. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7087b",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ddd; background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 8px; color: #333;\">\n",
    "\n",
    "## What improvements could be made? \n",
    "  \n",
    "The workflow that we have shown (data load -> train-test split -> build model) will be the same for nearly all machine learning applications, but there are many more advanced things that we might do along the way. Here, we talk about some of those\n",
    "\n",
    "### 1) Data Cleaning  \n",
    "  \n",
    "We have deliberately prepared a dataset for this task that is clean. In reality, this is nearly never the case. Data is messy; with errors, missing values and bias everywhere. A large part of the job of a data scientist is \"data cleaning\". This is the process of cleaning up the dataset by whatever means you can, so that it can be sensibly used in a machine learning application. This might include things like: \n",
    "* removing clear errors in the data \n",
    "* replacing missing values with some suitable alternative (maybe an average, or dropping them altogether)\n",
    "* visualising and exploring the data to check that it looks sensible \n",
    "* obtaining as much data as possible, to build better models\n",
    "\n",
    "### 2) Cross-Validation\n",
    "\n",
    "We mentioned earlier how important train-test splitting is for producing an unbiased estimate of how a model performs. It is *always* used. \n",
    "  \n",
    "However in many cases, this isn't really enough! We want to get confidence intervals on model performance, and understand how the model might perform if we used a different train-test split.  \n",
    "We do this with cross-validation. This is a way of training the model multiple times, using different splits of the data. Rather than getting one number for how a model performs, we get a distribution of values. We can use these to estimate things like the mean model performance, or how its performance might vary in different datasets. \n",
    "  \n",
    "In practice, we almost always use cross-validation when training models. It produces better estimates of model performance, especially when we only have a limited amount of data. \n",
    "  \n",
    "\n",
    "### 3) Hyperparameter Tuning\n",
    "  \n",
    "Earlier, we mentioned that machine learning algorithms have hyperparameters. These are parameters that the user sets that are not learned from the data (e.g. the number of trees in the random forest). Choosing sensible values of these is a critical part of building a machine learning model, and there are many different ways of doing it. We will not discuss these much here, but this article summarises some of the more important ones: https://www.geeksforgeeks.org/hyperparameter-tuning/\n",
    "\n",
    "### 4) Trying other models \n",
    "  \n",
    "We've talked about a random forest model; and for good reason, it is often considered \"good-enough\" for most use cases in machine learning on tabular data. \n",
    "  \n",
    "  \n",
    "*However, it is not the only model!*\n",
    "  \n",
    "  \n",
    "There are many, many different machine learning models that are all built on slightly different algorithms. A good practice for data science is to build different machine learning models to test the same problem. Other models may perform better.  \n",
    "\n",
    "### Closing Thoughts\n",
    "  \n",
    "The most important thing to remember is that data science is an *iterative* process. It involves progressively improving models, trying small changes and different techniques to try to produce the best model at the task you are dealing with. We hope in this course to give you an introduction to these ideas, but a lot of work is needed to use them in clinical practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07117c",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "[1]: Gillison, M. L., et al. (2000). Evidence for a causal association between human papillomavirus and a subset of head and neck cancers. *Journal of the National Cancer Institute*, 92(9), 709–720.  \n",
    "[2]: Ang, K. K., et al. (2010). Human papillomavirus and survival of patients with oropharyngeal cancer. *New England Journal of Medicine*, 363(1), 24–35.  \n",
    "[3]: Fakhry, C., et al. (2008). Improved survival of patients with human papillomavirus–positive head and neck squamous cell carcinoma in a prospective clinical trial. *Journal of the National Cancer Institute*, 100(4), 261–269.  \n",
    "[4]: Marur, S., et al. (2016). De-intensification of therapy in HPV-positive oropharyngeal cancer: ongoing clinical trials and future directions. *Oral Oncology*, 62, 50–56.  \n",
    "[5]: Lewis Jr, J. S., et al. (2012). Human papillomavirus testing in head and neck carcinomas: guideline from the College of American Pathologists. *Archives of Pathology & Laboratory Medicine*, 136(11), 1267–1277.  \n",
    "[6]: Tang, J., et al. (2013). A novel approach for classification of HPV-positive and HPV-negative head and neck squamous cell carcinomas based on RNA-seq data. *Bioinformatics*, 29(3), 275–281.  \n",
    "[7]: Seiwert, T. Y., et al. (2015). Integrative and comparative genomic analysis of HPV-positive and HPV-negative head and neck squamous cell carcinomas. *Clinical Cancer Research*, 21(3), 632–641.  \n",
    "[8]: Zhang, Y., et al. (2020). Machine learning algorithms for predicting HPV status from gene expression data. *BMC Bioinformatics*, 21(1), 1–13.  \n",
    "[9]: The Cancer Genome Atlas Network. (2015). Comprehensive genomic characterization of head and neck squamous cell carcinomas. *Nature*, 517(7536), 576–582.  \n",
    "[10]: Hoadley, K. A., et al. (2018). Cell-of-origin patterns dominate the molecular classification of 10,000 tumors from 33 types of cancer. *Cell*, 173(2), 291–304.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precision-course-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
